{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-D98CddQuwKG"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I8rGMlN2uzgk"
      },
      "outputs": [],
      "source": [
        # built on and edited  https://github.com/bhctsntrk/OpenAIPong-DQN to be a simpler implementation of Double DQN
        "ENVIRONMENT = \"PongDeterministic-v4\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "SAVE_MODELS = True  # Save models to file so you can test later\n",
        "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
        "SAVE_MODEL_INTERVAL = 50 # Save models at every X epoch\n",
        "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
        "\n",
        "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
        "LOAD_FILE_EPISODE = 0  # Load Xth episode from file\n",
        "\n",
        "BATCH_SIZE = 32 #64  # Minibatch size that select randomly from mem for train nets\n",
        "MAX_EPISODE = 15 #500 #900 #100000  # Max episode\n",
        "MAX_STEP = 5000 #100000  # Max step size for one episode\n",
        "\n",
        "MAX_MEMORY_LEN = 10000  # Max memory len\n",
        "MIN_MEMORY_LEN = 8000  # Min memory len before start train\n",
        "\n",
        "GAMMA = 0.97  # Discount rate\n",
        "ALPHA = 0.00025  # Learning rate\n",
        "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
        "\n",
        "RENDER_GAME_WINDOW = False  # Opens a new window to render the game (Won't work on colab default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HxF5-bzUu1q-"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Double DQN with:\n",
        "    - 3 layers of convolutional network\n",
        "    - 2 layers of fully connected network\n",
        "\n",
        "    Input: state\n",
        "    Output: Q-values for possible actions\n",
        "    \"\"\"\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        # Convolutional network layers with batch normalisation\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "\n",
        "        # Figuring out input size for fully-connected layers\n",
        "        linear_input_size = convw * convh * 64 \n",
        "\n",
        "        # Fully connected layers with ReLU activation function\n",
        "        self.fc1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.fc_relu = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=output_size)  # Q-value output\n",
        "        \n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        \"\"\"\n",
        "        Calculate the output image sizes before passing to FC layes\n",
        "        \"\"\"\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, state):\n",
        "        # forward pass: input to output propagation\n",
        "        state = F.relu(self.bn1(self.conv1(state)))\n",
        "        state = F.relu(self.bn2(self.conv2(state)))\n",
        "        state = F.relu(self.bn3(self.conv3(state)))\n",
        "\n",
        "        state = state.view(state.size(0), -1)  # flatten every batch to 2D\n",
        "        state = self.fc_relu(self.fc1(state))\n",
        "        q = self.fc2(state)  # outputs Q-values of actions\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "plT51MPbu5U5"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"\n",
        "        The agent that learns how to play Pong\n",
        "        \"\"\"\n",
        "        # State size before pre-processing\n",
        "        # Input size in Q-network\n",
        "        self.state_size_h = environment.observation_space.shape[0]\n",
        "        self.state_size_w = environment.observation_space.shape[1]\n",
        "        self.state_size_c = environment.observation_space.shape[2]\n",
        "\n",
        "        # Number of actions possible \n",
        "        # Output size in Q-network\n",
        "        self.action_size = environment.action_space.n\n",
        "\n",
        "        # State size after preprocessing\n",
        "        self.target_h = 84  # Height after process\n",
        "        self.target_w = 84  # Widht after process\n",
        "\n",
        "        # Removing scoreboards from frame (aka state)\n",
        "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  \n",
        "\n",
        "        \n",
        "        self.gamma = GAMMA  # Discount factor\n",
        "        self.alpha = ALPHA  # Learning rate\n",
        "\n",
        "        # decaying epsilon with a minimum\n",
        "        self.epsilon = 1  \n",
        "        self.epsilon_decay = EPSILON_DECAY  # decay rate\n",
        "        self.epsilon_minimum = 0.05 \n",
        "\n",
        "        # agent's replay memory\n",
        "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
        "        \n",
        "        # every tau  steps, copy online parameters for target\n",
        "        self.update_counter = 0  # Counter for steps\n",
        "        self.TARGET_UPDATE_INTERVAL = 1000  # tau\n",
        "\n",
        "        # Create two model for QNetwork algorithm\n",
        "        self.online_model = QNetwork(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model = QNetwork(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        \n",
        "        # ensure parameter of online is copied for target\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # Adam used as optimizer\n",
        "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
        "\n",
        "    def preProcess(self, image):\n",
        "        \"\"\"\n",
        "        Pre-process by image crop, resize, grayscale and normalize the images\n",
        "        \"\"\"\n",
        "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Turn image to grayscale from RGB\n",
        "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
        "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
        "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Get state, select and do action using epsilon-greedy policy\n",
        "        \"\"\"\n",
        "        if random.uniform(0, 1) <= self.epsilon:\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
        "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
        "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
        "\n",
        "        return action\n",
        "\n",
        "    def train(self,t): # add timestep\n",
        "        \"\"\"\n",
        "        Train neural nets with replay memory\n",
        "        Returns maximum Q-value and loss predicted from online network\n",
        "        \"\"\"\n",
        "        # if len(agent.memory) < MIN_MEMORY_LEN:\n",
        "        #     loss, max_q = [0, 0]\n",
        "        #     return loss, max_q\n",
        "        if t < MIN_MEMORY_LEN:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "\n",
        "        # We get out minibatch and turn it to numpy array\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
        "\n",
        "        # Concat batches in one array\n",
        "        # (np.arr, np.arr) ==> np.BIGarr\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        # Convert them to tensors\n",
        "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
        "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        # Estimate target Q\n",
        "        state_q_values = self.online_model(state)\n",
        "        next_states_q_values = self.online_model(next_state)\n",
        "        next_states_target_q_values = self.target_model(next_state)\n",
        "\n",
        "        # Find selected action's q_value\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Get indices of the max value of next_states_q_values\n",
        "        # Use that indice to get a q_value from next_states_target_q_values\n",
        "        next_states_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Use Bellman function to find target q value\n",
        "        target_q_value = reward + self.gamma * next_states_q_value * (1 - done)\n",
        "\n",
        "        # Calculate loss with target_q_value and q_value\n",
        "        loss = (selected_q_value - target_q_value.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), max_norm=1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_counter += 1\n",
        "\n",
        "        #only update target network after tau steps\n",
        "        if self.update_counter % self.TARGET_UPDATE_INTERVAL == 0:\n",
        "            self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "\n",
        "        return loss, torch.max(state_q_values).item()\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        \"\"\"\n",
        "        Store every transition to replay memory\n",
        "        \"\"\"\n",
        "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def decayingEpsilon(self):\n",
        "        \"\"\"\n",
        "        Decays the epsilon at every step to decrease exploration\n",
        "        \"\"\"\n",
        "        if self.epsilon > self.epsilon_minimum:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ve4vYDe3bozg"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(ENVIRONMENT)  # Initialise environment\n",
        "agent = Agent(environment)  # Create agent\n",
        "\n",
        "if LOAD_MODEL_FROM_FILE:\n",
        "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
        "\n",
        "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
        "        param = json.load(outfile)\n",
        "        agent.epsilon = param.get('epsilon')\n",
        "\n",
        "    startEpisode = LOAD_FILE_EPISODE + 1\n",
        "\n",
        "else:\n",
        "    startEpisode = 1\n",
        "\n",
        "last_100_ep_reward = deque(maxlen=50)  # Last 50 episode rewards\n",
        "last_100_ep_duration = deque(maxlen=50) # Last 50 episode durations\n",
        "total_step = 1  # Cumulative sum of all steps in episodes\n",
        "episode_rewards = []\n",
        "mean_eps_rewards =[]\n",
        "\n",
        "episodes =[]\n",
        "\n",
        "episode_durations = []\n",
        "mean_eps_durations = []\n",
        "\n",
        "episode_avg_max_q = []\n",
        "\n",
        "\n",
        "for episode in range(startEpisode, MAX_EPISODE):\n",
        "\n",
        "    startTime = time.time()  # Keep time\n",
        "    state, info = environment.reset()  # Reset env\n",
        "\n",
        "    state = agent.preProcess(state)  # Process image\n",
        "\n",
        "    # each state is a stack of 4 states\n",
        "    state = np.stack((state, state, state, state))\n",
        "\n",
        "    total_max_q_val = 0  # Total max q vals\n",
        "    total_reward = 0  # Total reward for each episode\n",
        "    total_loss = 0  # Total loss for each episode\n",
        "    for step in range(MAX_STEP):\n",
        "\n",
        "        if RENDER_GAME_WINDOW:\n",
        "            environment.render()  # Show state visually\n",
        "\n",
        "        # Select and perform an action\n",
        "        action = agent.act(state)  # Act\n",
        "        next_state, reward, done, trunc, info = environment.step(action)  # Observe\n",
        "\n",
        "        next_state = agent.preProcess(next_state)  # Process image\n",
        "\n",
        "        # each state is a stack of 4 states\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state  # Update state\n",
        "\n",
        "        if TRAIN_MODEL:\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            loss, max_q_val = agent.train(total_step)  # Train with random BATCH_SIZE state taken from mem\n",
        "        else:\n",
        "            loss, max_q_val = [0, 0]\n",
        "\n",
        "        total_loss += loss\n",
        "        total_max_q_val += max_q_val\n",
        "        total_reward += reward\n",
        "        total_step += 1\n",
        "        if total_step % 1000 == 0:\n",
        "            agent.decayingEpsilon()  # Decrase epsilon\n",
        "\n",
        "        if done or trunc:  # Episode completed\n",
        "            episodes.append(episode)\n",
        "            episode_rewards.append(total_reward)\n",
        "            episode_durations.append(step)\n",
        "\n",
        "            currentTime = time.time()  # Keep current time\n",
        "            time_passed = currentTime - startTime  # Find episode duration\n",
        "            current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
        "            epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
        "\n",
        "            if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
        "                weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
        "                epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
        "                rewardsPath = MODEL_PATH + str(episode) + '_rewards.npy'\n",
        "                mean_rewardsPath = MODEL_PATH + str(episode) + '_rewards.npy'\n",
        "\n",
        "                torch.save(agent.online_model.state_dict(), weightsPath)\n",
        "                with open(epsilonPath, 'w') as outfile:\n",
        "                    json.dump(epsilonDict, outfile)\n",
        "                np.save(rewardsPath, np.array(episode_rewards))\n",
        "                np.save(rewardsPath, np.array(mean_eps_rewards))\n",
        "\n",
        "            if TRAIN_MODEL:\n",
        "                agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
        "\n",
        "            last_100_ep_reward.append(total_reward)\n",
        "            mean_last_100_reward = np.mean(last_100_ep_reward)\n",
        "            mean_eps_rewards.append(mean_last_100_reward)\n",
        "\n",
        "            last_100_ep_duration.append(step)\n",
        "            mean_last_100_durations = np.mean(last_100_ep_duration)\n",
        "            mean_eps_durations.append(mean_last_100_durations)\n",
        "\n",
        "            avg_max_q_val = total_max_q_val / step\n",
        "            episode_avg_max_q.append(avg_max_q_val)\n",
        "\n",
        "            outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
        "                episode, current_time_format, total_reward, total_loss, mean_last_100_reward, avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
        "            )\n",
        "\n",
        "            print(outStr)\n",
        "\n",
        "            if SAVE_MODELS:\n",
        "                outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
        "                with open(outputPath, 'a') as outfile:\n",
        "                    outfile.write(outStr+\"\\n\")\n",
        "\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot learning curves\n",
        "std_rewards = np.std(mean_eps_rewards,axis=0)\n",
        "low_err = mean_eps_rewards-std_rewards\n",
        "upp_err = mean_eps_rewards+std_rewards\n",
        "#learning curve plotting\n",
        "plt.ion()  # Turn on interactive mode\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "# Plotting the initial graph with labels and title\n",
        "ax.set_title('Learning Curve')\n",
        "ax.set_xlabel('Episodes')\n",
        "ax.set_ylabel('Total Reward')\n",
        "ax.grid(True)\n",
        "ax.plot(episodes, episode_rewards, label='Total Reward', color = 'blue', linestyle = 'dashed', linewidth=1,)\n",
        "ax.plot(episodes, mean_eps_rewards, label='Mean Reward', color='orange',linewidth = 2)\n",
        "ax.fill_between(range(len(mean_eps_rewards)),low_err,upp_err,alpha = 0.25, color = 'orange',label='Standard Deviation')\n",
        "ax.legend(loc='best')\n",
        "plt.ioff()\n",
        "name = \"learning_curve_\"+str(MAX_EPISODE)+\".pdf\"\n",
        "plt.savefig(name,format=\"pdf\")\n",
        "plt.show()\n",
        "\n",
        "# # plot durations\n",
        "# std_duration = np.std(mean_eps_durations,axis=0)\n",
        "# low_err = mean_eps_durations-std_duration\n",
        "# upp_err = mean_eps_durations+std_duration\n",
        "# fig, ax = plt.subplots(figsize=(10, 6))\n",
        "# ax.set_title('Episode Durations')\n",
        "# ax.set_xlabel('Episodes')\n",
        "# ax.set_ylabel('Steps Taken')\n",
        "# ax.grid(True)\n",
        "# ax.plot(episodes, episode_durations, label='Total Steps', color = 'red', linestyle = 'dashed', linewidth=1,)\n",
        "# ax.plot(episodes, mean_eps_durations, label='Mean Steps', color='green',linewidth = 2)\n",
        "# ax.fill_between(range(len(mean_eps_rewards)),low_err,upp_err,alpha = 0.25, color = 'green',label='Standard Deviation')\n",
        "# ax.legend(loc='best')\n",
        "# plt.ioff()\n",
        "# name = \"episode_duration_\"+str(MAX_EPISODE)+\".pdf\"\n",
        "# plt.savefig(name,format=\"pdf\")\n",
        "# plt.show()\n",
        "\n",
        "# plot average maximum Q-value\n",
        "# std_q = np.std(mean,axis=0)\n",
        "# low_err = mean_eps_rewards-std_q\n",
        "# upp_err = mean_eps_rewards+std_q\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.set_title('Average Max Q-Value in Episode')\n",
        "ax.set_xlabel('Episodes')\n",
        "ax.set_ylabel('Avergae Maximum Q-value')\n",
        "ax.grid(True)\n",
        "ax.plot(episodes, episode_avg_max_q, label='Average Max Q-value', color = 'red', linestyle = 'dashed', linewidth=1,)\n",
        "# ax.fill_between(range(len(mean_eps_rewards)),low_err,upp_err,alpha = 0.25, color = 'green',label='Standard Deviation')\n",
        "ax.legend(loc='best')\n",
        "plt.ioff()\n",
        "name = \"avg_max_q_value_\"+str(MAX_EPISODE)+\".pdf\"\n",
        "plt.savefig(name,format=\"pdf\")\n",
        "plt.show()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "OpenAIPong-DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
